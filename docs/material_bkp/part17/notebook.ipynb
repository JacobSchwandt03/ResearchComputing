{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Systems for Data Science\n",
    "\n",
    "We introduce the concept of agents, **Multi-Agent Systems (MAS)**, and illustrate the potential of such systems in automating data science tasks, such as software development or data analysis. We use [OpenHands](https://github.com/All-Hands-AI/OpenHands) as our agentic software development example. For an example of multi-agent systems for cosmological data analysis, see [cmbagent](https://github.com/CMBagents/cmbagent).\n",
    "\n",
    "Useful references include:\n",
    "\n",
    "- [*Multi-Agent Reinforcement Learning: Foundations and Modern Approaches*, Albrecht, Christianos, Schafler (2024)](https://www.marl-book.com/)\n",
    "\n",
    "- [*AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework*, Wu et al (2023)](https://arxiv.org/abs/2308.08155)\n",
    "\n",
    "- [*OpenHands: An Open Platform for AI Software Developers as Generalist Agents*, Wang et al (2024)](https://github.com/All-Hands-AI/OpenHands)\n",
    "\n",
    "- [*Multi-Agent System for Cosmological Parameter Analysis*, Laverick et al (2024)](https://arxiv.org/pdf/2412.00431)\n",
    "\n",
    "- [Large Language Model Agents MOOC, Berkeley](https://llmagents-learning.org/f24)\n",
    "\n",
    "\n",
    "## Agency\n",
    "\n",
    "The English language has a great word (that doesn't exist in French): **agency**.  \n",
    "\n",
    "It means the *ability to act*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MASMARL.png\" alt=\"Agency\" width=\"500\">\n",
    "\n",
    "Figure from the [marl](https://www.marl-book.com/) book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above illustrates the concept of agency in multi-agent systems. There are two agents, that make observations and act on a joint environment. \n",
    "\n",
    "The particularity of agentic AI systemsis that the environment is acted upon, it changes. This is different from other types of AI systems you have seen so far for two reasons:\n",
    "\n",
    "1. The environment is static and only the model is updated.\n",
    "2. Once the model has been trained, the tasks that the model performs generally output an information that is completely seperate from the environment.\n",
    "\n",
    "For instance, when you train a neural network, the training data is fixed and only the weights and biases of the network are updated during learning/training. When you evaluate the network, the input data is also not changed but it simply output a prediction that lives somewhere else than the input data. \n",
    "\n",
    "When training a MAS, both the system and environment are updated. Moreover, from the perspective of each agent in the system, the environment is changed by the actions of the other agents. This is called **non-stationarity**.\n",
    "\n",
    "To understand this, take the example of a MAS for software development. The environment is the code base of the project. Agents can write, add, remove, modify, execute code. The environment, i.e., the code base, is changed by the actions of the agents. Training such a MAS means finding best communication protocols between agents, and for each agent the best instructions to follow, to successfully develop software that should be optimal in the sense that it meets the project requirements and some software quality metrics. Once the MAS is trained, it should be able to develop similar software by itself, optimally. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Basics \n",
    "\n",
    "\n",
    "\n",
    "Training a MAS is based on the principles of **Reinforcement Learning (RL)**. There is a direct parallel between RL and **Game Theory**.\n",
    "\n",
    "\n",
    "Let us introduce important concepts. As in other types of learnig, RL is dynamic and we use the notion of time step $t$.\n",
    "\n",
    "**Environment**: The environment is what the agents interact with. It is denoted by $\\mathcal{E}$. The interactions between the agents and the environment occur over a number of discrete time steps.\n",
    "\n",
    "\n",
    "**State**: At each time step $t$, an agent receives a state $s_t$. The set of possible states is denoted by $\\mathcal{S}$.\n",
    "\n",
    "**Action**: According to its policy, the agent in state $s_t$ chooses an action $a_t$ to take. The set of possible actions is denoted by $\\mathcal{A}$.\n",
    "\n",
    "\n",
    "**Policy**: The policy is the mapping from states to actions: \n",
    "\n",
    "$$\n",
    "\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}\n",
    "$$\n",
    "\n",
    "It is a strategy. It assigns probabilities to the available actions in each state.\n",
    "\n",
    "\n",
    "After an action is taken, the agent receives the next state $s_{t+1}$.\n",
    "\n",
    "**Reward**: The reward is a scalar value computed at each time step, indicating how good or bad the agent's last action was in achieving its goals. It is the feedback that the agent receives, to learn and improve its behavior (i.e., its policy) over time.\n",
    "\n",
    "\n",
    "For a one agent, the reward $r_t$ at time $t$ is defined as:\n",
    "\n",
    "$$\n",
    "r_t = R(s_t, a_t, s_{t+1}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $s_t \\in \\mathcal{S}$: The state of the environment at time $t$.\n",
    "\n",
    "- $a_t \\in \\mathcal{A}$: The action taken by the agent at time $t$.\n",
    "\n",
    "- $s_{t+1} \\in \\mathcal{S}$: The next state of the environment after taking action $a_t$.\n",
    "\n",
    "The **reward function** $R: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}$ maps the current state, action, and next state to a real number (the reward).\n",
    "\n",
    "\n",
    "For example, consider an agent controlling a robot trying to navigate a grid to reach a goal:\n",
    "\n",
    "- States ($\\mathcal{S}$): The robot's position on the grid.\n",
    "\n",
    "- Actions ($\\mathcal{A}$): {Up, Down, Left, Right}.\n",
    "\n",
    "- Reward ($R$):\n",
    "\n",
    "  - $+10$: If the robot reaches the goal state.\n",
    "\n",
    "  - $-1$: For each move (to encourage efficiency).\n",
    "\n",
    "  - $-100$: If the robot collides with an obstacle.\n",
    "\n",
    "If the robot is at position $(1, 1)$, moves \"Right\" to $(1, 2)$, and hits an obstacle, the environment provides a reward $r_t = -100$.\n",
    "\n",
    "\n",
    "\n",
    "**Markov decision process (MDP)**: The standard model to define the environment in which the agent chooses actions over a number of time steps.\n",
    "\n",
    "\n",
    "Useful references:\n",
    "\n",
    "- [*Reinforcement Learning: An Introduction*, Sutton and Barto (2020)](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1ARL.png\" alt=\"MDP\" width=\"500\">\n",
    "\n",
    "Figure from [Sutton and Barto (2020)](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf). Basic RL loop for a 1-agent system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "Useful references:\n",
    "\n",
    "- [*Asynchronous Methods for Deep Reinforcement Learning*, Mnih et al (2016)](https://arxiv.org/abs/1602.01783)\n",
    "\n",
    "- [CambMARL](https://github.com/borisbolliet/CambMARL)\n",
    "\n",
    "A pilot course on deep RL may be offered in the second semester (see with Boris Bolliet for details)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
